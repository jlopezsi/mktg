{
    "contents" : "---\ntitle: 'Segmentación de mercados: proceso'\nauthor: \"Jordi López Sintas\"\ndate: \"9 de enero de 2015\"\noutput: pdf_document\n---\n\n#Modelos de segmentación: proceso\n\n1\\. El procedimiento estándar\n\nFase exploratoria: la clasificación jerárquica\n\nLa clasificación jerárquica con el procedimiento de Ward minimiza la suma de las distancias euclidianas al cuadrado entre el individuo i y el centro del segmento al que se asigna,$\\sum_{\\substack{k}}\\sum_{\\substack{i \\in k}}\\sum_{\\substack{j}}=(x(i,j)-x(j',j))^2$. El primer sumatorio calcula el error de la observación *i* en el segmento *k* para todas las variables, el segundo realiza el cálculo para todos los individuos en el segmento *k* y, finalmente, el último sumatorio calcula la suma del error para todos los segmentos (Wishart 1998).\n\nFunciona de la siguiente manera\n\n1.  Computa la matriz de las distancias euclidianas al cuadrado para\n    todo par de observaciones, *i*, *i’*, $d^2$(*i*,*i’*)=$\\sum_{\\substack{k}}(x(i,j)-x(j',j))^2$.\n\n2.  Agrupa las dos observaciones o grupos *i* y *i’* más cercanos, es\n    decir, cuya agrupación minimiza el incremento en el error, *E*.\n    Inicialmente serán aquellos casos cuya distancia, $d^2 (i, i’)$,\n    sea mínima.\n3.  Transforma la matriz inicial de distancias euclidianas al cuadrado\n    $D^2$ en $E^2$, la cual contendrá el error al cuadrado de la\n    unión del nuevo segmento $i \\cup i$.\n\n4.  Repite los pasos 2 y 3 y cada vez forma un nuevo grupo con las\n    observaciones o grupos cuya unión resulte en un incremento mínimo en\n    el error $E^2$.\n\n5.  Finaliza cuando todos los casos se hayan agrupado en un solo\n    segmento\n\nEl procedimiento minimiza el cuadrado de la suma de las desviaciones\nentre los individuos y el centro del grupo al que se le ha asignado.\nRealiza un proceso de aglomeración con tantas fases o etapas como\nindividuos ha de clasificar, *n*, de manera que una vez clasificados en\nsu totalidad se minimiza una medida de la heterogeneidad, $min \\sum_{SQD-I}$. En la etapa inicial existen tantos grupos como individuos\ndebe agrupar. A partir de ese momento en cada etapa se formará un nuevo\nsegmento agrupando a dos de los segmentos ya formados en etapas\nanteriores o un grupo y un individuo aun no clasificado, aquéllos más\nparecidos entre sí, de forma que se minimice en cada agrupación el\nincremento en la suma de las diferencias entre el individuo agrupado y\nla media del grupo al que se asigna, $min\\left\\{\\sum_{\\substack{i}}{x^2(i,j)}-\\frac{\\sum_{\\substack{j}}x^2(i,j)}{n}\\right\\}$. En la etapa inicial la suma del\ncuadrado de las distancias es cero y se va incrementando a medida que se\nrealizan las agrupaciones.\n\nSuponogamos que tenemos cinco individuos medidos en una única variable $\\left\\{A=2,B=5,C=9,D=10,E=15\\right\\}$.\n\nd(A, B)=22+52-(1/2)(2+5)2=29-24,5=4,5\n\n      A   B     C      D           E\n  --- --- ----- ------ ----------- ------\n  A   0   4,5   24,5   32          98\n  B       0     8      12,5        60,5\n  C             0      **0,5**   24,5\n  D                    0           18\n  E                                0\n\nLa agrupación C y D es la que minimiza el SQD.\n\n       A   B           CD   E\n  ---- --- ----------- ---- -------\n  A    0   **4,5**   38   98\n  B        0           14   60,5\n  CD                   0    28,66\n  E                         0\n\nLa agrupación A y B es la que minimiza la SQD\n\n       AB   CD   E\n  ---- ---- ---- -------------\n  AB   0    41   108,66\n  CD        0    **28,66**\n  E              0\n\nLa agrupación *CDE* es la que minimiza la *SQD*. Finalmente *d*(*AB*,\n*CDE*)= 113,2.\n\nLa agrupación se muestra visualmente con un gráfico denominado\ndendrograma, donde las letras identifican a los individuos.\n```{r}\nejemplo<-c(2,5,9,10,15)\nejemplo\nejemplo.dist<-dist(ejemplo)\nejemplo.dist\nejemplo.hclust<-hclust(ejemplo.dist, method=\"ward\")\nejemplo.hclust\nplot(ejemplo.hclust)\nlabels<-c(\"A\", \"B\", \"C\", \"D\", \"E\")\nejemplo.hclust$labels<-labels\nplot(ejemplo.hclust)\n```\n\nVeamos cómo lo haríamos en el entorno R de análisis de datos.\n\n#Partición de la muestra en k segmentos: el procedimiento k-means\n\nLos algoritmos de clasificación del tipo k-means inician el proceso de\nselección especificando el número de clases que desea el analista, *k*,\nasí como los centros iniciales para cada uno de los *k* clusters a\nformar. Después asigna cada individuo al grupo cuyo centro tenga más\npróximo. La solución inicial se mejora de manera iterativa hasta que se\nalcanza alguna forma de estabilidad, por ejemplo, hasta que las\nreasignaciones no reducen la suma del cuadrado de las distancias de los\nconsumidores a los centros de los segmentos a los que han sido asignados\n(Helsen y Green 1991). Concretamente:\n\n1.  Empieza el proceso con un conjunto de centros iniciales o\n    coordenadas de los *k* segmentos. Estos centros pueden obtenerse de\n    manera aleatoria o por algún procedimiento como el propuesto por\n    Hartigan y Wong (1979).\n\n2.  Después asignamos al consumidor *i* al cluster cuyo centro está más\n    próximo. Los centros permanecen inalterados durante todo el proceso\n    de asignación.\n\n3.  Seguidamente calculamos un nuevo conjunto de centros de los\n    segmentos, como la media de los consumidores asignados a cada uno.\n    Estos nuevos centros serán la base de un nuevo ciclo de\n    reasignaciones.\n\n4.  Repetimos los procesos 2 y 3 hasta que ningún individuo cambie de\n    grupo en el proceso de asignación especificado en el paso 2.\n\nEn este algoritmo el centro de cada segmento se define como la media de\ntodos los individuos asignados al segmento en cada una de las variables\nen las que se han medido sus propiedades. Por ello necesita una matriz de datos *n*x*p* en lugar de una matriz de\ndistancias entre los individuos (a diferencia de lo que ocurre en los\nalgoritmos de clasificación jerárquica). El propósito del algoritmo es\nminimizar la suma de las distancias euclidianas al cuadrado de la\npartición realizada en *k* segmentos. Implícitamente asume que los\ngrupos muestran una distribución normal esférica. Vemos un ejemplo.\n\n*n*= número de individuos a clasificar\n\n*p*= número de variables en las que se han medido las propiedades de los\nindividuos\n\n*x*(*i,j*)= valor que muestra el individuo *i* en la variable *j*\n\n*x*(*l,j*)= valor medio de la variable *j* en el segmento *l*.\n\n*n*(*l*)=número de individuos en el grupo *l*.\n\n*d*(*i,l*)= distancia entre el individuo *i* y el centro del segmento\n*l*.\n\n*d*(*i,l*)=raíz cuadra de la suma para toda *p* de las diferencias al\ncuadrado\n\n*e*(*p*(*n, k*))=error de la partición\n\n*p*(*n, k*)= resultado de partir la muestra en *k* segmentos y asignar a\nlos *n* individuos a cada uno de los *k* segmentos.\n\n*min* *e*(*p*(*n, k*))=$\\sum_{\\substack{i=1,...,n}}$*d^2^*(*i,l*(*i*))=*min* *SQD~I~*\n\n  Tipo de pescado   Energía   Calorías   Calcio   Sum(i)\n  ----------------- --------- ---------- -------- --------\n  Caballa           5         9          20       34\n  Perca             6         11         2        19\n  Salmón            4         5          20       29\n  Sardina           6         9          46       61\n  Atún              5         7          1        13\n  Camarones         3         1          12       16\n\nSupongamos que queremos formar tres segmentos. El procedimiento nos\ndaría como resultado los siguientes segmentos:\n\nSegmento 1: perca, atún y caballa\n\nSegmento 2: caballa y salmón\n\nSegmento 3: sardina\n\nSeguidamente calcularemos la media de las propiedades de los objetos\nclasificados en cada uno de los segmentos.\n\n  Segmento   Energía   Calorías   Calcio\n  ---------- --------- ---------- --------\n  1          14/3      19/3       5\n  2          9/2       7          20\n  3          6         9          46\n\nY calculamos la distancia euclidiana entre los individuos y las medias\nde cada grupo:\n\nE(p(n=6,k=3))=SQDI=$d^2$^(1,1)+$d^2$(2,1)+…+$d^2$(6,1)+…+$d^2$(1,3)+…+$d^2$(6,3)=137,805\n\nSeguidamente probamos si cualquier cambio en la asignación de individuos\na los segmentos reduce el error de la clasificación o suma del cuadrado\nde las distancias entre individuos y centros. Siendo *n*(*l*) el número\nde individuos asignados al segmento *l*, y *l*(*i*) el segmento que\ncontiene al individuo *i*, primero calculamos las distancias al cuadrado\nentre el primer individuos y los centros de cada uno de los grupos:\n\n$d^2$(1,1)=$(5-14/3)^2$+$(9-19/3)^2$+$(20-5)^2$=232,22\n\n$d^2(1,2)=4,25$\n\n$d^2(1,2)=677$\n\nA la hora de decidir donde clasificar al individuo i, calcularemos la\nvariación en la *SQD~I~*. En este caso, cambiar la caballa de segmento\nincrementaría el error de la partición realizada.\n\nRealizamos el proceso para todos los objetos y encontramos, en este\ncaso, que el objeto 6, el camarón, puede ser clasificado en el segmento\n2, en lugar del *1* inicial, y reducir el error de la clasificación.\nQuedando así la clasificación siguiente:\n\nSegmento 1: perca y atún\n\nSegmento 2: caballa, salmón y camarón\n\nSegmento 3: sardinas.\n\nCódigo (modificar)\n\n\n\nEnergia Calorias Calcio\n\nCaballa 5 9 20\n\nPerca 6 11 2\n\nSalmon 4 5 20\n\nSardina 6 9 46\n\nAtun 5 7 1\n\nCamarones 3 1 12\n```{r}\npeces<-read.csv(\"peces.csv\", row.names=1, header=T)\npeces\npeces.dist<-dist(peces)\n\npeces.dist\n```\n\nCaballa Perca Salmon Sardina Atun\n\nPerca 18.138357\n\nSalmon 4.123106 19.078784\n\nSardina 26.019224 44.045431 26.381812\n\nAtun 19.104973 4.242641 19.131126 45.055521\n\nCamarones 11.489125 14.456832 9.000000 35.057096 12.688578\n\n\nYou can also embed plots, for example:\n\n```{r, echo=FALSE}\npeces.hclust<-hclust(peces.dist, method=\"ward\")\nplot(peces.hclust)\n```\nPartimos la muestra en tres segmentos\n```{r, echo=FALSE}\npeces.kmeans<-kmeans(peces, 3, trace=T)\n```\nSi queremos saber las opciones de lal función k-means, podemos utilizar la función ? seguida del nombre de la función\n```{r, echo=TRUE}\n?kmeans\n```\nPara saber los objetos incluidos en el resultado de clasificar la muestra, podemos uitlizar la función names()\n```{r}\nnames(peces.kmeans)\n```\nLos centros de los segmentos se encuentran en el objeto `centers`. La classificación se encuentra en el objeto `cluster`, y en el objeto `iter`el número de iteraciones realizadas.\n\n```{r}\npeces.kmeans$centers\npeces.kmeans$cluster\npeces.kmeans$iter\n```\nEl resultado lo podemos visualizar en el espacio de las bases de segmentación\n```{r}\nplot(peces[1:2], col=peces.kmeans$cluster)\npoints(peces[1:2])\npoints(peces.kmeans$centers, col=1:2, pch=8,cex=2)\n```\n\n\n",
    "created" : 1420817451572.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "597438021",
    "id" : "691D3433",
    "lastKnownWriteTime" : 1420822724,
    "path" : "~/Documents/github-projects/mktg/segmentation.procedure.v2.Rmd",
    "project_path" : "segmentation.procedure.v2.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}