---
title: 'Caso KcHospital: Analysis en r'
output: pdf_document
---

#Lectura de los datos del caso

La lectura de datos en el lenguage **R** es muy flexible. Aunque existen funciones para la lectura de dtos procedentes d eotros programas de análisi estadístico, una formaestándar es la lectura de dtos en formato ASCII separados con tabulador. Si tenemos los datos en otro programa de análisis de datos o en una hoja electrónica Excel, sólo tendremos que grabarlos en formato texto con separción de tabulaciones El fichero se grabará con la extensión .dat o .txt, como es el caso del ficheor de datos del hopsital KC ("hospital/hospital.txt") y las preferencias de los consumidores entrevistados ("hospital/hospital.prefs.txt"). Para su lectura en el entorno **R** utilizamos la función `read.table`. Como argumentos utilizamos el nombre y ubicación del fichero de datos, entrcomillado, y la indicación de que la primera fila del ficheor contiene los nombres de las varaibles (`header=T`).

```{r}
setwd("/Users/jlopezsi/Documents/github/mktg-models-projects")
kchospital <- read.table("hospital/hospital.txt", header = TRUE)
kcprefs <- read.table("hospital/hospital.prefs.txt", header = TRUE)
```
Ahora que ya los tenemos en le espacio de trabajo podemos consultar nombre de las variables contenidas en la tabla de datos kchospital que acabamos de crear. Para ello utilizamos la función `names` y como argumento el nombre de la tabal de datos, kchospital o kcprefs, sin comillas. Aquí tenemos el nombre de las variables contenidas en ambos ficheros.

```{r}
names(kchospital)
names(kcprefs)
```
Si queremos ver el contenido de cualquier variables debemos hacer referencia a la tabla de datos que lo contiene. Por ejemplo, `kchospital$FIREP` nos permite acceder al contenido de la variable `reputación` que está en la tabla de datos `kchospital`. Cuidado, en **R** se diferencia entre las mayúsculas y las minúsculas. Por ello deberemos introducir los nombre de los ficheros y variables en minúsculas o mayúsculas según sea el caso. Aquí el nombre de la tabla de datos está en minúsculas y las variables en mayúsculas; así debemos hacer referencia a ellas.

Para facilitar el trabajo con la hoja de datos podemos adjuntarla al espacio de trabajo mediante la función `attach()`. De esa manera no tendremos que hacer referencia a la tabla de datos `kchospital`para acceder al contenido de las variables.

```{r, echo=TRUE}
attach(kchospital)
```

#Bases de segmentación

para facilitar el análisis de segmentación primero vamos a seleccionar las bases de segmentación, las variables que los consumidores consideran importantes a la hora de seleccionar un hospital materno-infantil. Concretamente,

```{r}
bases <- kchospital[,c(10:20)]
names(bases)
```

Ahora debemos cargar en la memoria del ordenador la bibliteca de los paquetes que necesitamos para realizar el análisis de segmentación. Concretamente, necesitamos la bilbioteca `cluster`. Las otras funciones utilizadas en la segmentación están disponibles en el programa base de **R**. Para cargar los paquetes utilizamos la función `library()` y como argumento el nombre del paquete. El paque `cluster` contiene un conjunto de algoritmos de clasificación robustos propuestos pro Kauffman y Rouseeuw (1990). Los podemos agrupar en tres tipos: algoritmos para partir un conjunto de observaciones (`pam`, `clara`, y `fanny`), algoritmos de aglomeración jerárquica (`agnes`) y, finalmente, alforitmos para la división jerárquica de un conjunto de aobseraciones (`diana` y `mona`).  

```{r}
library(cluster)
```

#Proceso de segmentación en dos etapas: aglomeración jerárquica y partición

Vamos a utilizar, en primer lugar, los algoritmos clásicos implementados en los programas de análisis de datos más habituales: el jerárquico aglomerativo, hclust, y el de partición de una muestra, kmeans. Si queremos saber algo más sobre el algoritmo implementado podemos utilizar la ayuda, help, y como argumento en nombre de la función de la que queremos saber algo, hclust,como argumento. Por ejemplo, `help(hclust)`.  Con la función `scale(fi)`podemos normalizar los datos si es preciso.

##clasificación con los datos originales

Para ello seguimos el proceso usual. Primero, analizamos visualmente la heterogeneidad que existe en la muestra de clientes o posibles clientes –para ello utilizamos un procedimiento jerárquico de aglomeración, `hclust`–, segundo, decidimos cuántos segmentos formar y aplicamos en procedimiento de partición del mercado, `kmeans`. 

###Exploración de la heterogeneidad en los datos

Veamos el resultado de la clasificación jerárquica.

```{r}
bases.hclust<-hclust(dist(bases), method="ward")
plot(bases.hclust)
plot(bases.hclust, main="Dendrograma",ylab="distancia",xlab="objetos clasificados",frame.plot=TRUE)
```

Aunque el dendrograma (que es como se denomina al gráfico en forma de árbol invertido que muestra el proceso de clasificación) ya es lo suficientemente elegante, podemos añadirle un título y modificar el nombre de los ejes del gráfico. Si queremos resaltar el número de grupos que formaríamos, podemos utilizar la siguiente función. Si fueran tres los segmentos a formar, obtendríamos:


```{r}
plot(bases.hclust, main="Dendrograma",ylab="distancia",xlab="objetos clasificados",frame.plot=TRUE)
rect.hclust(bases.hclust, k=3, border=2)
```

Si formaramos cuatro segmentos, entonces utilizaríamos:

```{r}
plot(bases.hclust, main="Dendrograma",ylab="distancia",xlab="objetos clasificados",frame.plot=TRUE)
rect.hclust(bases.hclust, k=4, border=2)
```

###Partición con kmeans

Durante la segunda fase, después de decidir cuántos grupos formaremos, utilizaremos un procedimiento de partición, concretamente `Kmeans`.

```{r}
bases.kmeans3<-kmeans(bases, 3)
```

Ayudas visuales para analizar la estructura de los datos. Sólo si las bases de segmentación están muy correlacionadas. En caso contrario tendremos que basarnos en el dendograma que ya hemos obtenido.

Si, además, hemos obtenido los componentes principales, podemos visualizar el resultado de la clasificación. Primero obtenemos los componentes principales y después utilizamos éstos para visualizar la segmentación realizada

```{r}
library(MASS)
bases.pca<-princomp(bases, cor=T)
bases.puntos<-predict(bases.pca)
plot(bases.puntos[,1:2], col=bases.kmeans3$cluster)
```

Repetimos el proceso con cuatro grupos:

```{r}
bases.kmeans4<-kmeans(bases, 4)
plot(bases.puntos[,1:2], col=bases.kmeans4$cluster)
```

Visualmente podemos ver que el algoritmo nos ha dividido un grupo que parece homogéneo, mientras que el grupo que parece formado por dos segmentos lo ha dejado igual. Esto es debido a que el algoritmo `kmeans` forma grupos esféricos.

###¿Podemos mejorar la clasificación realizada?

En ciertas ocasiones podemos mejorar la clasificación facilitando al algoritmo `kmeans` los centros iniciales a partir de los cuales iniciar la partición. Para ello tendremos que calcular la media de cada variable en cada uno de los grupos formados con el procedimiento jerárquico de aglomeración. Veamos cómo obtener las medias:

```{r}
centros4<-tapply(as.matrix(bases), list(rep(cutree(bases.hclust, 4), ncol(as.matrix(bases))),col(as.matrix(bases))),mean)
options(digits=3)
centros4
```

Ahora repetimos el proceso y visualizamos el resultado.

```{r}
bases.kmeans4<-kmeans(bases, centros4)
plot(bases.puntos[,1:2], col=bases.kmeans4$cluster)
```

##¿Existe correlación entre las variables? Cambio de variables o construcción de las nuevas bases de segmentación

La existencia de correlación entre las variables podría ser uno de los motivos por los cuales el procedimiento no funcione correctamente. Averigüemos si las correlaciones son elevadas.

```{r}
cor(bases)
```

Si existe correlación entre las variables que los consumidores consideran importantes a la hora de escoger un hospital, puede que la clasificación obtenida no sea la correcta. En este caso debemos eliminar la correlación existente entre las variables utilizadas. Para ello podemos utilizar una técnica ampliamente utilizada para la reducción de datos, el análisis de los componentes principales. Esta técnica crea unas nuevas variables a partir de una combinación lineal de las originales. Estas nuevas variables, componentes principales, poseen dos características muy útiles en el análisis de datos multivariantes: no están correlacionadas entre sí y nos explican la correlación que se observa entre las variables originales. 

Para la extracción de los componentes principales que existen en la matriz de datos de factores importantes, bases, utilizaremos la biblioteca de programas de Venables y Ripley (1999), MASS.

```{r}
library(MASS)
bases.pca<-princomp(bases, cor=T)
summary(bases.pca)
bases.puntos<-predict(bases.pca)

```

Para continuar con el análisis primero decidiremos con cuántos componentes principales trabajaremos. Para ello pediremos que nos muestre gráficamente la cantidad de varianza que las nuevas variables son capaces de explicar (función `screepot(fipca)` con el objeto creado como argumento). Podemos ser que con tres es suficiente pues a partir de cuarto componente la varianza explicada es inferior a la unidad (varianza de una variable original estandarizada) y se observa una inflexión en el gráfico.

```{r}
screeplot(bases.pca)
```

Ahora vamos a crear una nueva variable (una matriz de datos) con los puntos asignados a los clientes de la muestra en las nuevas variables o componentes principales (podemos utilizar predict(fipca) o bien fipca$scores, en ambos casos obtenemos el mismo resultado).

```{r}
bases.puntos<-predict(bases.pca)
```

Una ventaja de trabajar con un espacio reducido de variables es poder visualizar cómo se distribuyen los clientes que queremos clasificar. Si formamos un gráfico con los dos primeros componentes (representan casi el 78% de la variabilidad de la muestra) podremos visualmente ver la formación de los grupos. La función `eqscplot` nos los permite hacer fácilmente. El gráfico nos muestra que existen 4 grupos, dos de ellos grandes con un valor reducido en el segundo componente, y dos, reducidos ambos, con un valor elevador en el segundo componente y medio en el primero.

```{r}
eqscplot(bases.puntos, xlab="primer componente", ylab="segundo componente")
```

Si queremos guardar estos tres componentes para futuros análisis, podemos asignarlos a tres nuevas variables de la hoja de datos.

```{r}
eqscplot(bases.puntos, xlab="primer componente", ylab="segundo componente")
```

Si queremos guardar estos tres componentes para futuros análisis, podemos asignarlos a tres nuevas variables de la hoja de datos.

##Repetimos el proceso de clasificación con las nuevas bases de segmentacion

De nuevo vamos a utilizar los algoritmos clásicos implementados en los programas de análisis de datos más habituales: el jerárquico, `hclust`, y el de partición `kmeans`. 

Obtendremos un nuevo objeto de la clase ``hclust`, bases.puntos.hclust, cuyo proceso de aglomeración podemos observar gráficamente gracias a la función `plot` que precisa como argumento el nombre  del objeto que acabamos de crear, bases.puntos.hclust. Las distancias verticales nos dan una medida visual de la heterogeneneidad que existe en los grupos. Un examen visual nos indica que pueden haber 3 o 4 grupos, dos grandes y dos pequeños que podrían agruparse en uno solo, aunque la varianza dentro de los grupos sería bastante mayor que la correspondiente a una agrupación con 4.

```{r}
bases.puntos.hclust<-hclust(dist(bases.puntos[,1:3]), method="ward")
plot(bases.puntos.hclust)
```

Si queremos saber a qué grupos pertenecen cada uno de los clientes podemos utilizar la función cutree, nos asigna cada caso a un grupo.

```{r}
cutree(bases.puntos.hclust, 4)
```

Para tener que escribir menos podemos almacenar los tres componentes en una nueva tabla de datos, bases.puntos3:

```{r}
bases.puntos3<-bases.puntos[,1:3]
head(bases.puntos3)

plot(bases.puntos.hclust, main="Dendrograma PCA",labels=FALSE,ylab="distancia",xlab="objetos clasificados",frame.plot=TRUE)
rect.hclust(bases.puntos.hclust, k=4, border=2)
```

¿Cuántos grupos formar? Vemos que al utilizar los componentes principales el dendograma nos sugiere que es mayor la heterogeneidad entre los dos segmentos más pequeños (la longitud de la línea que los une es mayor). Por ello tentativamente formaremos 4 grupos.

Ahora realizaremos la clasificación con el algoritmo `kmeans`. El resultado de este algoritmo es muy sensible a los centros iniciales a partir de los cuales inicia la partición. Veámoslo, en este caso es así. 

Primero realizaremos la clasificación con unos centros obtenidos aleatoriamente y el resultado lo asignaremos al objeto `bases.puntos.kmeans`. La asignación de cada individuo a las particiones obtenidas se encuentra almacenado en el atributo `cluster` del objeto `bases.puntos.kmeans`, y podemos acceder a ella de la forma habitual, `bases.puntos.kmeans$cluster`. 

```{r}
bases.puntos.kmeans<-kmeans(bases.puntos3, 4)
bases.puntos.kmeans$cluster
```

Representación gráfica del resultado con el algoritmo kmeans con centros aleatorios, incluidos los centros finales de los 4 grupos o particiones realizadas. Utilizaremos al función plot y como argumento el nombre del objeto

```{r}
plot(bases.puntos[,1:2], col=bases.puntos.kmeans$cluster)
```

Además, podemos añadir al gráfico los centros de cada segmento

```{r}
#points(bases.puntos.kmeans$centers, col=bases.puntos.kmeans$cluster, pch=8)
```

##¿Podemos mejorar la partición utilizando los centros iniciales proporcionados por el algoritmo de aglomeración jerárquica?

Veamos, ahora, el resultado de la partición si les damos como centros los obtenidos en la clasificación con el método de Ward. Para su cálculo utilizamos el procedimiento propuesto por Venables y Ripley (1999: 338). 

```{r}
centros.puntos4<-tapply(bases.puntos3, list(rep(cutree(bases.puntos.hclust, 4),ncol(bases.puntos3)),col(bases.puntos3)),mean)
centros.puntos4
```

Volvemos a clasificar los individuos con el procedimiento kmeans y asignamos el resultado al objeto fikmeanspca2. Fíjate, ahora, que no es necesario el argumento correspondiente al número de grupos a formar, pues este dato lo obtiene del número de filas de la matriz de medias, el objeto identificado con el nombre de centros.

```{r}
bases.puntos.kmeans4.c<-kmeans(bases.puntos3, centros.puntos4)


```

Volvamos a representar gráficamente el resultado de la clasificación contenida en el objeto fikmeanspca3. 

```{r}

plot(bases.puntos[,1:2], col=bases.puntos.kmeans4.c$cluster)
points(bases.puntos.kmeans4.c$centers,col=1:2, pch=8)

```

#Procedimientos más robustos: el paquete `cluster`

##Partición con las variables originales

Realizamos la primera segmentación con los datos originales. Veamos ahora los resultados del programa `cluster`. Para ello empleamos el procedimiento de partición denominado `clara`: cuyo significado es  *clustering large applications*. La ventaja del paquete de procedimiento resarrollado por Kaufman y Rouseeuw (1990) reside en que es menos sensible que *Kmeans* cuando los segmentos no tienen una forma esférica y, además, nos proporciona un índice de la calidad de la partición obtenida. Veámoslo:

```{r}
bases.cluster<-clara(bases, 2, metric="euclidean")
bases.cluster3<-clara(bases, 3, metric="euclidean")
bases.cluster4<-clara(bases, 4, metric="euclidean")
bases.cluster5<-clara(bases, 5, metric="euclidean")
clusplot(bases.cluster)
clusplot(bases.cluster3)
clusplot(bases.cluster4)
clusplot(bases.cluster5)
```

Repetimos la operación con 3, 4 y 5 particiones obteniendo los siguientes índices de calidad de: 0,53, 0,63, y 0,42, respectivamente para 2, 3, 4 y 5 particiones. Los resultados nos indican que la mejor clasificación se obtiene con 3 particiones como se observa también la representación gráfica en un espacio reducido mostrada por la función `clusplot`.  `Clara` con 3 clusters sólo tiene un error al fusionar los clusters 3 y 4.


```{r}
names(bases.cluster)
bases.cluster$silinfo$avg.width
bases.cluster3$silinfo$avg.width
bases.cluster4$silinfo$avg.width
bases.cluster5$silinfo$avg.width
```

Si le padimos los atributos del objeto de la clase cluster que acabamos de obtener (con la función `names(bases.cluster))` nos indica cómo podemos acceder de forma individual a cada uno de los apartador que contiene el resumen, los argumentos disponibles son los siguientes: *sample*, *medoids*, *clustering*, *objective*, *clusinfo*, *silinfo*, *diss* y *data*. Para acceder directamente al resultado de la clasificación podemos utilizar la misma estructura que utilizamos en una tabla de datos, el nombre de objeto, `bases.cluster`, seguido del nombre del argumento al cual deseamos acceder, `clustering`, ambos conectados con el símbolo del dólar, *bases.cluster$clustering*.

Si queremos obtener ayuda sobre alguna función o biblioteca de programas incorporada a nuestra lenguaje **R**, podemos utilizar la función `help.start()` y sin argumento, paréntesis vacíos.

##¿Podemos confiar en el resultado obtenido? ¿Existe correlación entre las variables?

Veamos ahora el resultado de la clasificación utilizando el algoritmo `clara` con los  tres componente principales estimados. Creamos un nuevo objeto, bases.puntos.cluste, con 2, 3, 4 y 5 grupos, de forma sucesiva. El índice de calidad obtenido es de: 0,69, 0,77, 0,81 y 0,68; con lo que los resultados sugieren que cuatro grupos es la mejor solución. Evidentemente, la representación gráfica de los grupos obtenidos (función `clusplot(bases.puntos.cluster)`) únicamente en la partición en cuatro grupos éstos se muestran totalmente separados sin solapamiento alguno. En cambio, en el resto de las soluciones obtenidas algunos grupo se encuentran solapados, reduciendo el índice de calidad de la estructura obtenida.

```{r}
#con componentes principales

bases.puntos.cluster<-clara(bases.puntos[,1:3],2,metric="euclidean")
bases.puntos.cluster3<-clara(bases.puntos[,1:3],3,metric="euclidean")
bases.puntos.cluster4<-clara(bases.puntos[,1:3],4,metric="euclidean")
bases.puntos.cluster5<-clara(bases.puntos[,1:3],5,metric="euclidean")

clusplot(bases.puntos.cluster)
clusplot(bases.puntos.cluster3)
clusplot(bases.puntos.cluster4)
clusplot(bases.puntos.cluster5)

bases.puntos.cluster$silinfo$avg.width
bases.puntos.cluster3$silinfo$avg.width
bases.puntos.cluster4$silinfo$avg.width
bases.puntos.cluster5$silinfo$avg.width
```

Guardamos el resultado de la clasificación con 4 segmentos y realizamos una representación gráfica en el espacio de los primeros componentes principales del resultado obtenido con las variabels originales y después con los componentes principales (comparar con el resultado obtenido con 3 segmentos.

```{r}
kchospital$clusterpca<-bases.puntos.cluster4$clustering
plot(bases.puntos[,1:2], col=bases.cluster4$clustering)
plot(bases.puntos[,1:2], col=bases.puntos.cluster4$clustering)

```

¿Hay diferencias? Veamos, ahora, el resultado de la clasificación realizada por Kmeans con las variables originales (sin sustituirlas por los componentes principales) y el obtenido con clara. ¿Existen diferencias?

Si queremos exportar los  resultados, podemos utilizar las siguientes funciones:

```{r}
#write.table(kchospital, "kcclas.dat", quote = FALSE, sep = ",", row.names=F)
```

#Identificación de los consumidores asignados a cada segmento

Las variables despriptoras suelen ser variables nomnales con diferentes niveles. Si la base de datos no contiene las etiquetas de los niveles, primero deberemos preparar la base de datos como sigue, por ejemplo.

```{r}
names(kchospital)

library(plyr)
library(dplyr)
descriptores<-kchospital[,c(43:47)]
head(descriptores) 

class(descriptores$SEXO)
descriptores$SEXO<-as.factor(descriptores$SEXO)
summary(descriptores$SEXO)

class(descriptores$ESTADOMA)
descriptores$ESTADOMA<-as.factor(descriptores$ESTADOMA)
summary(descriptores$ESTADOMA)

class(descriptores$EDAD)
descriptores$EDAD<-as.factor(descriptores$EDAD)
summary(descriptores$EDAD)

class(descriptores$NIVELEDU)
descriptores$NIVELEDU<-as.factor(descriptores$NIVELEDU)
summary(descriptores$NIVELEDU)

class(descriptores$INGRESOS)
descriptores$INGRESOS<-as.factor(descriptores$INGRESOS)
summary(descriptores$INGRESOS)

descriptores$SEXO <- revalue(descriptores$SEXO, c("0" ="Mujer", "1" = "Hombre"))
descriptores$ESTADOMA <- revalue(descriptores$ESTADOMA, c("0" ="Casado", "10"="Soltero", "1" = "Otro"))
descriptores$EDAD <- revalue(descriptores$EDAD, c("0" ="<=25 años", "1"="55+", "10"="41-55", "100" = "26-40"))
descriptores$NIVELEDU <- revalue(descriptores$NIVELEDU, c("0" ="ESO", "1"="Graduado Univ.", "10"="Bachillerato +", "100" = "Bachillerato"))
descriptores$INGRESOS <- revalue(descriptores$INGRESOS, c("0" ="<=$100000", "1"="$60000+", "10"="$40001-60000", "100" = "$30001-40000", "1000"="$20001-30000", "10000"= "$10001-20000"))


```

Despues elaboramos las tablas que nos describen los segmentos

```{r}
options(digits=3)
descriptores.tabla<- list( 
t(prop.table(table(bases.puntos.cluster4$clustering, descriptores$SEXO))),
t(prop.table(table(bases.puntos.cluster4$clustering, descriptores$ESTADOMA))),
t(prop.table(table(bases.puntos.cluster4$clustering, descriptores$EDAD))),
t(prop.table(table(bases.puntos.cluster4$clustering, descriptores$NIVELEDU))),
t(prop.table(table(bases.puntos.cluster4$clustering, descriptores$INGRESOS)))
)
do.call(rbind, descriptores.tabla)
```

#Informe 

Problema de marketing: ¿Debería el hopital comercializar un servici especializado en maternidad para incrementar la tasa de ocupación?


En términos comerciales:
Determinar la facilidad con la que los consumidores identifican los hospitales, sus preferencias e intenciones de compra para el servicio de maternidad.

Componentes específicos del problema comercial

1. ¿Qué criterios utilizan los consumidores cuando seleccionan un hospital? 
2. ¿Cuál e sla posicón relativa de Kc en comparación con la competencia? 
3. ¿Qué competidores tienen el mejor servicio de maternidad según sus percepciones? 
4. ¿Cuál es el mercado potencial del servicio de maternidad? ¿Quienes son en términos demográficos y psicológicos? 
5. ¿Y el mercado potencial de KC? 
6. ¿Podemos incrementar la tasa de reconocimienot del hospital con la introducción del servicio de maternidad u otro servicio especializado? 


```{r}
#library(DiagrammeR)
#diagrama.causalidad <-DiagrammeR("
#  graph BT;
#    Cuidado.especializado --> Ventaja.competitiva;
#    Ventaja.competitiva --> Reconocimiento;
#    Reconocimiento -->Preferencia;
#    Preferencia --> Tasa.de.ocupacion;
#    Tasa.de.ocupacion--> Beneficio;
#")
#diagrama.causalidad
```

Componentes principales (Q2A-J)
Reducir componentes principales y ver si existe heterogeneidad respecto a los últimos hospitales visitados (Q1).
Explicar solución PCA

PCA 1 "fundamentos del cuidado". En un extremo variables relacionadas con la reputación, y en el otro, los aspectos sociales del cuidado.

PCA 2  "aspectos comerciales", correlaciones positcias para "advertising," y "acceptance of maternity insurance." 

PCA 3 "proximidad" dimensión correlacionada con "distance from home."

Cluster: Una solución con 4 segmentos:

Cluster 
1 96 
2 13
3 133 
4 28

Según la dimensión de los segmentos se podrían agrupar en tres segmentos.  

Describir segmentos según su importancia en los factores de elección del hospital.

Cluster 1 precoupado por los fundamentos del cuidado

Cluster 2 preocupado por la proximidad. 

Cluster 3 este grupo se deja aconsejar por los conocidos y no le importan los aspectos comerciales. 

cluster 4 is high on factor 2, meaning this group is responsive to business decisions related to maternity care.

Si tabulamos la clasificación en segmentos con los resultados de la primera pregunta, tenemos que una correspondencia clara  entre el uso previo y la asginación a los clusters:  cluster 1 corresponde a los hospitales  B, C, F; cluster 2 a KC hospital; cluster 3 ...

EXECUTIVE SUMMARY

Los resultados muestran que la experiencia previa en hosptales es el criterio más importante para los consumidors cuando deciden a qué hospital ir. Otros criterios importantes son: la calidad del servicio, las promociones que incrementan el valor, y la proximidad geográfica. Por ello ofrecer  un servicio de matenidad para conseguir que los consumidores repitan en su elección de hospital parece una bena idea.

Las características que el servicio debe son varias. Primero la atención personal fue un criterio importante para los consumidores con mayor nivel educativo e ingresos superiores. Segundo estos consumidores tambien perciben que deben tener libertad para escoger hospital, mientras que los consumidores con bajos ingresos y educación inferior creen que no tienen elección. Adicionalmente, los grupos más privilegiados socialmente tambien ven la publiciadad como algo positivio, indicando que la publicidad puede influir en su elección.

Además, los consumidores familiares con los servicios de matenidad creen que la comodidad es una caracteritica deseable un un servicio de matenidad. Por ello, la atención personal junto con los fundamentos del cuidado profesional y la comodidad deberían resaltar en la publicidad que intenta ganar clientes para el servicio de maternidad de KC. Esa publicia, junto con el servicio de maternidad, incrementaría el nivel de reconocimiento de la marca e incrementaria el valor de su imagen (en ambos está por detras de sus competidores)



